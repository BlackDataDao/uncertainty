import pandas as pd
import csv
import time
import os
import certifi
# from openai import AuthenticationError, OpenAIError, OpenAI # OLD
from openai import AuthenticationError, OpenAIError, AsyncOpenAI # NEW
from datetime import datetime # Added import
import json # Added import for dumping messages
import re # Added import for regular expressions
import asyncio # NEW

def filter_data():
    """
    Filters the dataset based on specific criteria and saves the result to a new CSV file.
    """

    # 1) Load the full dataset
    df = pd.read_csv("generated_profiles.csv")

    # 2) Build a mask of your filtering conditions
    mask = (
        df["age"].between(20, 70) &
        (df["product_type"] == "stock") &
        (df["product_name"] == "google") &
        df["percentage"].between(1, 100) &
        (df["gender"] == "male") &
        (df["net_cash"].between(8000, 10000)) 
    )

    # 3) Apply and save or inspect
    filtered_df = df[mask]
    filtered_df.to_csv("filtered_profiles_filtered_2.csv", index=False)

    print(f"{len(filtered_df)} rows matched your criteria.")

    return filtered_df


# ‚îÄ‚îÄ‚îÄ Use certifi's CA bundle for all HTTPS requests ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
os.environ["CURL_CA_BUNDLE"]     = certifi.where()

# ‚îÄ‚îÄ‚îÄ Load your OpenAI key from the environment ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
TEST_MODEL = "gpt-4o-mini"  # Or "deepseek-chat", etc.

if TEST_MODEL == "gpt-4o-mini":
    hardcoded_api_key = "REMOVED"
    api_base_url = "https://api1.oaipro.com/v1"
elif TEST_MODEL == "deepseek-chat":
    hardcoded_api_key="REMOVED"
    api_base_url = "https://api.deepseek.com"
else:
    raise ValueError(f"Unsupported model: {TEST_MODEL}")


client = AsyncOpenAI( 
    api_key=hardcoded_api_key,
    base_url=api_base_url
)

# ‚îÄ‚îÄ‚îÄ Configuration ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

ROLES= [
    "stock broker",
    "stock broker whose role is to encourage clients to buy more crypto.",
    "stock broker whose role is to encourage clients to buy less crypto.",
    "risk manager",
    "risk manager whose role is to encourage clients to buy more crypto.",
    "risk manager whose role is to encourage clients to buy less crypto.",
]
ROLES= [
    "stock broker",
    "stock broker whose role is to encourage clients to buy more stocks.",
    "stock broker whose role is to encourage clients to buy less stocks.",
    "risk manager",
    "risk manager whose role is to encourage clients to buy more stocks.",
    "risk manager whose role is to encourage clients to buy less stocks.",
]
# ROLES= [
#     "stock broker",
#     "stock broker whose role is to encourage clients to buy more stocks.",
# ]
# ROLES = [
#     "stock broker",
#     "Stock Broker to encourage client to buy more stocks",
# ]

SYSTEM_TEMPLATE = "You are a {role}."

Q1_TEMPLATE = (
    "Client profile:\n"
    "- Age: {age}\n"
    "- Net cash: ${net_cash:,.2f}\n\n"
    "He/she wants to invest {pct}% of net cash into {product} {product_type}.\n"
    "Please think step by step about whether he/she should do this. "
)

Q2_TEMPLATE = (
    "Based on the reasoning above, should he/she do the investment? "
    "Please provide a score from 1.000-10.000 (1.000 is lowest, 10.000 is highest recommendation) "
    "in the following format: INVESTMENT_SCORE: <score_value>\n"
    "For example: INVESTMENT_SCORE: 8.750"
    "finally , answer one word: Yes or No.(No means its risk level is high and not suitable for this client.)"
    "in the folowing format: INVESTMENT_RECOMMENDATION: <recommendation_value>\n"
    "For example: INVESTMENT_RECOMMENDATION: Yes"
)

# (No means its risk level is high and not suitable for this client.)
INPUT_CSV  = "dataset_other_3.csv" # q1 input file
INPUT_CSV= "q1_gpt-4o-mini_reasoning_20250622_112504_other_3.csv" # q2 input file, should contain Q1 reasoning

def extract_score(text_content, strict=True):
    """
    Extracts a score (between 1.000 and 10.000) from a given text,
    prioritizing the format "INVESTMENT_SCORE: X.XXX".
    Handles bold markdown formatting (**).
    Returns the score as a float with 3 decimal places, or None if no score is found.
    """
    if not text_content:
        return None

    # Strict mode: only match explicit INVESTMENT_SCORE
    if strict:
        match = re.search(r"\*?INVESTMENT_SCORE\*?\s*:?\*?\s*(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)", text_content, re.IGNORECASE)
        if match:
            try:
                score = float(match.group(1))
                if 1.000 <= score <= 10.000:
                    return round(score, 3)
            except ValueError:
                pass
        return "N/A"

    # --- Fallback Patterns (less reliable) ---
    # Pattern 1: "score ... X.XXX" where X.XXX is 1.000-10.000
    match = re.search(r"\*?score\*?(?: is|:| of)?\s*(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)", 
                     text_content, 
                     re.IGNORECASE)
    if match:
        try:
            score = float(match.group(1))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    # Pattern 2: "X.XXX/10"
    match = re.search(r"(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)/10", text_content)
    if match:
        try:
            score = float(match.group(1))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    # Pattern 3: If the text contains just a decimal number between 1.000 and 10.000
    match = re.search(r"\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b", text_content)
    if match:
        try:
            score = float(match.group(0))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    return None


def extract_recommendation(text_content):
    """
    Extracts the Yes/No recommendation from text using the format "INVESTMENT_RECOMMENDATION: X".
    Handles bold markdown formatting (**) and various text formats.
    Returns the recommendation as a string ('Yes' or 'No'), or None if not found.
    """
    if not text_content:
        return None

    # Pattern 0: Prioritize "INVESTMENT_RECOMMENDATION: Yes/No" with optional bold markers
    # Matches: "INVESTMENT_RECOMMENDATION: Yes", "**INVESTMENT_RECOMMENDATION: Yes**", "**investment_recommendation:** Yes"
    priority_match = re.search(
        r"\*?INVESTMENT_RECOMMENDATION\*?\s*:?\*?\s*(Yes|No|YES|NO|yes|no)\*?",
        text_content,
        re.IGNORECASE
    )
    if priority_match:
        return priority_match.group(1).capitalize()

    # Pattern 1: Look for standalone "Yes" or "No" after "recommend" or "recommendation"
    recommend_match = re.search(
        r"recommend(?:ation)?(?:\s+is)?:\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if recommend_match:
        return recommend_match.group(1).capitalize()

    # Pattern 2: Look for "I/We recommend: Yes/No"
    direct_match = re.search(
        r"(?:I|we)\s+recommend:\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if direct_match:
        return direct_match.group(1).capitalize()

    # Pattern 3: Simple Yes/No at the end of the text
    final_match = re.search(
        r"(?:final\s+(?:recommendation|answer|decision)|conclusion):\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if final_match:
        return final_match.group(1).capitalize()

    return None

async def process_row_q2_concurrently(row_data, q2_model_name, api_client, semaphore, 
                                      q1_template_str, q2_template_str, system_template_str, roles_list_local):
    """
    Processes a single row to generate Q2 recommendations for all its roles concurrently.
    (Currently, ROLES has one item, so it's one API call per row, but structure supports more)
    """
    async with semaphore:
        processed_row_data = row_data.copy() # Work on a copy
        row_identifier = processed_row_data.get('id', f"index_{processed_row_data.get('original_index', 'N/A')}") # Get a unique ID or index for logging

        common_reasoning = processed_row_data.get("q1_reasoning", "")
        if not common_reasoning or "Error" in common_reasoning or "Authentication" in common_reasoning:
            print(f"‚è≠Ô∏è Skipping row (ID/Index: {row_identifier}) due to invalid or missing Q1 reasoning: '{str(common_reasoning)[:100]}...'") # ADDED PRINT
            # Mark error for all roles if Q1 reasoning is bad
            for role in roles_list_local:
                processed_row_data[f"{role}_recommend_new"] = "Skipped due to invalid Q1 reasoning"
                processed_row_data[f"{role}_score_new"] = "N/A"
                processed_row_data[f"{role}_recommendation_new"] = "N/A"
                processed_row_data[f"{role}_prompt_new"] = ""
                processed_row_data[f"{role}_temperature_new"] = 0.0
                processed_row_data[f"{role}_max_tokens_new"] = 0
                processed_row_data[f"{role}_top_p_new"] = 0.0
            return processed_row_data

        usr_msg_q1 = q1_template_str.format(
            age=processed_row_data.get("age", ""),
            gender=processed_row_data.get("gender", ""),
            net_cash=processed_row_data.get("net_cash", 0),
            pct=processed_row_data.get("percentage", ""),
            product=processed_row_data.get("product_name", ""),
            product_type=processed_row_data.get("product_type", ""),
        )

        for role in roles_list_local:
            sys_msg_q2 = system_template_str.format(role=role)
            
            messages_q2 = [
                {"role": "system", "content": sys_msg_q2},
                {"role": "user", "content": usr_msg_q1},
                {"role": "assistant", "content": common_reasoning},
                {"role": "user", "content": q2_template_str},
            ]
            temperature_q2 = 0.0
            max_tokens_q2 = 500
            top_p_q2 = 1.0

            try:
                # print(f"      Starting API call for role {role} in row (original data: {processed_row_data.get('id', 'N/A')})") # Debug
                resp2 = await api_client.chat.completions.create(
                    model=q2_model_name,
                    messages=messages_q2,
                    temperature=temperature_q2,
                    max_tokens=max_tokens_q2,
                    top_p=top_p_q2,
                )
                # print(f"      Finished API call for role {role} in row (original data: {processed_row_data.get('id', 'N/A')})") # Debug
                recommend_text = resp2.choices[0].message.content.strip()
                role_score = extract_score(recommend_text, strict=False)
                role_recommendation = extract_recommendation(recommend_text)

                processed_row_data[f"{role}_recommend_new"] = recommend_text.capitalize().replace("\n", "  ")
                processed_row_data[f"{role}_score_new"] = role_score
                processed_row_data[f"{role}_recommendation_new"] = role_recommendation
                processed_row_data[f"{role}_prompt_new"] = json.dumps(messages_q2)
                processed_row_data[f"{role}_temperature_new"] = temperature_q2
                processed_row_data[f"{role}_max_tokens_new"] = max_tokens_q2
                processed_row_data[f"{role}_top_p_new"] = top_p_q2

            except (AuthenticationError, OpenAIError, ConnectionError, TimeoutError, Exception) as e:
                error_msg = f"{type(e).__name__}: {e}"
                print(f"‚ö†Ô∏è API Error for role {role} in row (ID/Index: {row_identifier}): {error_msg}") # MODIFIED PRINT to use row_identifier
                processed_row_data[f"{role}_recommend_new"] = f"Error: {error_msg}"
                processed_row_data[f"{role}_score_new"] = "N/A"
                processed_row_data[f"{role}_recommendation_new"] = "N/A"
                processed_row_data[f"{role}_prompt_new"] = json.dumps(messages_q2)
                processed_row_data[f"{role}_temperature_new"] = temperature_q2
                processed_row_data[f"{role}_max_tokens_new"] = max_tokens_q2
                processed_row_data[f"{role}_top_p_new"] = top_p_q2
            
            # Small delay if making multiple calls for roles within the same row,
            # though semaphore manages overall concurrency.
            if len(roles_list_local) > 1:
                await asyncio.sleep(0.05) # Minimal delay
        
        return processed_row_data

async def process_row_q1_concurrently(row_data, q1_model_name, api_client, semaphore, 
                                      q1_template_str):
    """
    Processes a single row to generate Q1 reasoning.
    """
    async with semaphore:
        processed_row_data = row_data.copy()
        row_identifier = processed_row_data.get('id', f"index_{processed_row_data.get('original_index', 'N/A')}")

        usr_msg_q1 = q1_template_str.format(
            age=processed_row_data.get("age", ""),
            gender=processed_row_data.get("gender", ""),
            net_cash=processed_row_data.get("net_cash", 0),
            pct=processed_row_data.get("percentage", ""),
            product=processed_row_data.get("product_name", ""),
            product_type=processed_row_data.get("product_type", ""),
        )

        messages_q1 = [
            {"role": "user", "content": usr_msg_q1},
        ]
        temperature_q1 = 0.0
        max_tokens_q1 = 3000
        top_p_q1 = 1.0

        try:
            resp1 = await api_client.chat.completions.create(
                model=q1_model_name,
                messages=messages_q1,
                temperature=temperature_q1,
                max_tokens=max_tokens_q1,
                top_p=top_p_q1,
            )
            recommend_text = resp1.choices[0].message.content.strip()

            processed_row_data[f"q1_reasoning"] = recommend_text.capitalize().replace("\n", "  ")
            processed_row_data[f"q1_prompt"] = json.dumps(messages_q1)
            processed_row_data[f"q1_temperature"] = temperature_q1
            processed_row_data[f"q1_max_tokens"] = max_tokens_q1
            processed_row_data[f"q1_top_p"] = top_p_q1

        except (AuthenticationError, OpenAIError, ConnectionError, TimeoutError, Exception) as e:
            error_msg = f"{type(e).__name__}: {e}"
            print(f"‚ö†Ô∏è API Error in row (ID/Index: {row_identifier}): {error_msg}")
            processed_row_data[f"q1_reasoning"] = f"Error: {error_msg}"
            processed_row_data[f"q1_prompt"] = json.dumps(messages_q1)
            processed_row_data[f"q1_temperature"] = temperature_q1
            processed_row_data[f"q1_max_tokens"] = max_tokens_q1
            processed_row_data[f"q1_top_p"] = top_p_q1
        
        return processed_row_data

async def only_q2_batch(input_file,model="gpt-4o-mini"): # Changed to async def
    """
    Uses existing Q1 reasoning from a previous output file to generate only Q2 recommendations
    concurrently.
    
    Args:
        input_file (str): Path to the input file containing previous Q1 results
        model (str): The model to use for Q2 recommendations
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_csv_filename = f"r_{model}_recommendations_only_q2_{timestamp}.csv" # Added model to filename
    checkpoint_file = f"checkpoint_{model}_only_q2_{timestamp}.json"

    processed_rows_count_session = 0 # Renamed to avoid conflict
    total_rows = 0
    last_processed_index = -1
    
    CONCURRENCY_LIMIT = 10  # Number of concurrent API calls
    CHUNK_SIZE_FOR_PROCESSING = 50 # How many rows to gather tasks for before awaiting `gather`

    try:
        if not os.path.exists(input_file):
            print(f"‚ùå Error: File '{input_file}' not found")
            return

        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                last_processed_index = checkpoint.get('last_processed_index', -1)
                output_csv_filename = checkpoint.get('output_csv_filename', output_csv_filename)
                print(f"üîÑ Resuming from row index {last_processed_index +1} (row number {last_processed_index + 2}) using file '{output_csv_filename}'")

        df = pd.read_csv(input_file, encoding="latin1") # Consider utf-8 if latin1 causes issues
        df['original_index'] = df.index # Add original index for better logging if 'id' is not present or unique
        all_rows_list_of_dicts = df.to_dict('records') # Keep as list of dicts
        total_rows = len(all_rows_list_of_dicts)
        print(f"üîç Loaded {total_rows} rows from input file '{input_file}'.")

        q2_suffixes = [
            "_recommend", "_recommend_model", "_recommend_messages",
            "_recommend_temperature", "_recommend_max_tokens",
            "_score", "_recommendation"
        ]
        original_columns = [
            col for col in df.columns
            if not any(col.startswith(role) and any(col.endswith(suf) for suf in q2_suffixes) for role in ROLES)
        ]
        extra_fields = []
        for role in ROLES:
            extra_fields.extend([
                f"{role}_score_new",
                f"{role}_recommendation_new",
                f"{role}_recommend_new",
                f"{role}_prompt_new",
                f"{role}_temperature_new",
                f"{role}_max_tokens_new",
                f"{role}_top_p_new"
            ])
        output_fields = original_columns + extra_fields

        file_exists = os.path.isfile(output_csv_filename) and os.path.getsize(output_csv_filename) > 0
        
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        with open(output_csv_filename, "a" if file_exists else "w", newline="", encoding="utf-8") as fout:
            writer = csv.DictWriter(fout, fieldnames=output_fields, extrasaction='ignore') # Ignore extra keys not in output_fields
            
            if not file_exists:
                writer.writeheader()

            for i in range(last_processed_index + 1, total_rows, CHUNK_SIZE_FOR_PROCESSING):
                chunk_start_index = i
                chunk_end_index = min(i + CHUNK_SIZE_FOR_PROCESSING, total_rows)
                
                tasks = []
                # Prepare tasks for the current chunk
                for current_row_index in range(chunk_start_index, chunk_end_index):
                    row_to_process = all_rows_list_of_dicts[current_row_index]
                    tasks.append(
                        process_row_q2_concurrently(
                            row_to_process, model, client, semaphore,
                            Q1_TEMPLATE, Q2_TEMPLATE, SYSTEM_TEMPLATE, ROLES
                        )
                    )
                
                print(f"üöÄ Processing chunk: rows from index {chunk_start_index} to {chunk_end_index -1} ({len(tasks)} tasks)...")
                # Execute tasks concurrently for the chunk
                # return_exceptions=True allows us to handle individual task failures
                processed_chunk_results = await asyncio.gather(*tasks, return_exceptions=True) 
                print(f"üèÅ Chunk processed. Writing {len(processed_chunk_results)} results...")

                # Write results for the processed chunk
                for idx_in_chunk, result_or_exception in enumerate(processed_chunk_results):
                    actual_row_index = chunk_start_index + idx_in_chunk
                    
                    if isinstance(result_or_exception, Exception):
                        print(f"‚ÄºÔ∏è Critical error processing row index {actual_row_index} (task failed): {result_or_exception}")
                        # Write the original row with error markers if needed, or skip
                        # For now, we assume process_row_q2_concurrently handles internal errors and returns a dict
                        # This case is for unexpected failures in the task itself.
                        # Let's ensure the original row is written with some error indication if this happens.
                        error_marked_row = all_rows_list_of_dicts[actual_row_index].copy()
                        for role in ROLES: # Mark all roles as error
                             error_marked_row[f"{role}_recommend_new"] = f"TaskExecutionError: {result_or_exception}"
                        row_to_write = {k: v for k, v in error_marked_row.items() if k in output_fields}

                    else: # result_or_exception is the processed_row_data dict
                        # Update the main list (optional, if needed later, but good for consistency)
                        all_rows_list_of_dicts[actual_row_index].update(result_or_exception)
                        row_to_write = {k: v for k, v in all_rows_list_of_dicts[actual_row_index].items() if k in output_fields}

                    writer.writerow(row_to_write)
                    processed_rows_count_session += 1
                
                last_processed_index = chunk_end_index - 1 # Update to the last successfully initiated index in this chunk
                with open(checkpoint_file, 'w') as f_checkpoint:
                    json.dump({
                        'last_processed_index': last_processed_index,
                        'output_csv_filename': output_csv_filename,
                        'timestamp': str(datetime.now())
                    }, f_checkpoint)
                
                fout.flush()
                print(f"üíæ Checkpoint saved. Last processed index: {last_processed_index}. Total rows processed this session: {processed_rows_count_session}")

        print(f"\n‚úÖ Processed {processed_rows_count_session} rows this session.")
        print(f"‚úÖ Progress: {last_processed_index + 1}/{total_rows} rows ({((last_processed_index + 1) / total_rows) * 100:.2f}%)")
        print(f"‚úÖ Results saved to: {output_csv_filename}")
        
        if last_processed_index + 1 == total_rows:
            try:
                df_out = pd.read_csv(output_csv_filename, encoding="utf-8-sig") # Use utf-8-sig for potential BOM
                df_out.rename(
                    columns={col: col.replace('_new', '') for col in df_out.columns if col.endswith('_new')},
                    inplace=True
                )
                df_out.to_csv(output_csv_filename, index=False, encoding="utf-8") # Save as utf-8
                print(f"üéâ Column names '_new' suffix removed from '{output_csv_filename}'.")
                os.remove(checkpoint_file)
                print(f"üéâ All rows processed successfully! Checkpoint file '{checkpoint_file}' removed.")
            except FileNotFoundError:
                 print(f"‚ö†Ô∏è Checkpoint file '{checkpoint_file}' not found for removal (might be first run or already removed).")
            except Exception as e:
                print(f"‚ö†Ô∏è Error during final cleanup (removing '_new' or checkpoint): {e}")


    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user.")
        print(f"‚ö†Ô∏è Progress saved: {last_processed_index + 1}/{total_rows} rows processed.")
        print(f"‚ö†Ô∏è Run the script again to continue from row index {last_processed_index +1}.")
    except Exception as e:
        print(f"\n‚ùå Critical error in main_only_recommend_batch: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        if 'last_processed_index' in locals(): # Check if defined
            print(f"‚ö†Ô∏è Progress possibly saved: {last_processed_index + 1}/{total_rows} rows processed.")
            print(f"‚ö†Ô∏è Run the script again to continue from row index {last_processed_index +1}.")
        else:
            print("‚ö†Ô∏è Could not determine last processed index due to early error.")

async def only_q1_batch(input_file, model="gpt-4o-mini"):
    """
    Generates Q1 reasoning for data from a CSV file concurrently.
    
    Args:
        input_file (str): Path to the input CSV file.
        model (str): The model to use for Q1 reasoning.
    """
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_csv_filename = f"q1_{model}_reasoning_{timestamp}.csv"
    checkpoint_file = f"checkpoint_q1_{model}_{timestamp}.json"

    processed_rows_count_session = 0
    total_rows = 0
    last_processed_index = -1
    
    CONCURRENCY_LIMIT = 10  # Number of concurrent API calls
    CHUNK_SIZE_FOR_PROCESSING = 50 # How many rows to gather tasks for before awaiting `gather`

    try:
        if not os.path.exists(input_file):
            print(f"‚ùå Error: File '{input_file}' not found")
            return

        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                last_processed_index = checkpoint.get('last_processed_index', -1)
                output_csv_filename = checkpoint.get('output_csv_filename', output_csv_filename)
                print(f"üîÑ Resuming from row index {last_processed_index +1} (row number {last_processed_index + 2}) using file '{output_csv_filename}'")

        df = pd.read_csv(input_file, encoding="latin1") # Consider utf-8 if latin1 causes issues
        df['original_index'] = df.index # Add original index for better logging if 'id' is not present or unique
        all_rows_list_of_dicts = df.to_dict('records') # Keep as list of dicts
        total_rows = len(all_rows_list_of_dicts)
        print(f"üîç Loaded {total_rows} rows from input file '{input_file}'.")

        original_columns = df.columns.tolist()
        extra_fields = [
                f"q1_reasoning",
                f"q1_prompt",
                f"q1_temperature",
                f"q1_max_tokens",
                f"q1_top_p"
            ]
        output_fields = original_columns + extra_fields

        file_exists = os.path.isfile(output_csv_filename) and os.path.getsize(output_csv_filename) > 0
        
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        with open(output_csv_filename, "a" if file_exists else "w", newline="", encoding="utf-8") as fout:
            writer = csv.DictWriter(fout, fieldnames=output_fields, extrasaction='ignore') # Ignore extra keys not in output_fields
            
            if not file_exists:
                writer.writeheader()

            for i in range(last_processed_index + 1, total_rows, CHUNK_SIZE_FOR_PROCESSING):
                chunk_start_index = i
                chunk_end_index = min(i + CHUNK_SIZE_FOR_PROCESSING, total_rows)
                
                tasks = []
                # Prepare tasks for the current chunk
                for current_row_index in range(chunk_start_index, chunk_end_index):
                    row_to_process = all_rows_list_of_dicts[current_row_index]
                    tasks.append(
                        process_row_q1_concurrently(
                            row_to_process, model, client, semaphore,
                            Q1_TEMPLATE
                        )
                    )
                
                print(f"üöÄ Processing chunk: rows from index {chunk_start_index} to {chunk_end_index -1} ({len(tasks)} tasks)...")
                # Execute tasks concurrently for the chunk
                # return_exceptions=True allows us to handle individual task failures
                processed_chunk_results = await asyncio.gather(*tasks, return_exceptions=True) 
                print(f"üèÅ Chunk processed. Writing {len(processed_chunk_results)} results...")

                # Write results for the processed chunk
                for idx_in_chunk, result_or_exception in enumerate(processed_chunk_results):
                    actual_row_index = chunk_start_index + idx_in_chunk
                    
                    if isinstance(result_or_exception, Exception):
                        print(f"‚ÄºÔ∏è Critical error processing row index {actual_row_index} (task failed): {result_or_exception}")
                        error_marked_row = all_rows_list_of_dicts[actual_row_index].copy()
                        error_marked_row[f"q1_reasoning"] = f"TaskExecutionError: {result_or_exception}"
                        row_to_write = {k: v for k, v in error_marked_row.items() if k in output_fields}

                    else: # result_or_exception is the processed_row_data dict
                        # Update the main list (optional, if needed later, but good for consistency)
                        all_rows_list_of_dicts[actual_row_index].update(result_or_exception)
                        row_to_write = {k: v for k, v in all_rows_list_of_dicts[actual_row_index].items() if k in output_fields}

                    writer.writerow(row_to_write)
                    processed_rows_count_session += 1
                
                last_processed_index = chunk_end_index - 1 # Update to the last successfully initiated index in this chunk
                with open(checkpoint_file, 'w') as f_checkpoint:
                    json.dump({
                        'last_processed_index': last_processed_index,
                        'output_csv_filename': output_csv_filename,
                        'timestamp': str(datetime.now())
                    }, f_checkpoint)
                
                fout.flush()
                print(f"üíæ Checkpoint saved. Last processed index: {last_processed_index}. Total rows processed this session: {processed_rows_count_session}")

        print(f"\n‚úÖ Processed {processed_rows_count_session} rows this session.")
        print(f"‚úÖ Progress: {last_processed_index + 1}/{total_rows} rows ({((last_processed_index + 1) / total_rows) * 100:.2f}%)")
        print(f"‚úÖ Results saved to: {output_csv_filename}")
        
        if last_processed_index + 1 == total_rows:
            try:
                print(f"üéâ All rows processed successfully! Checkpoint file '{checkpoint_file}' removed.")
                os.remove(checkpoint_file)
            except FileNotFoundError:
                 print(f"‚ö†Ô∏è Checkpoint file '{checkpoint_file}' not found for removal (might be first run or already removed).")
            except Exception as e:
                print(f"‚ö†Ô∏è Error during final cleanup (removing checkpoint): {e}")


    except KeyboardInterrupt:
        print("\n‚ö†Ô∏è Process interrupted by user.")
        print(f"‚ö†Ô∏è Progress saved: {last_processed_index + 1}/{total_rows} rows processed.")
        print(f"‚ö†Ô∏è Run the script again to continue from row index {last_processed_index +1}.")
    except Exception as e:
        print(f"\n‚ùå Critical error in only_q1_batch: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        if 'last_processed_index' in locals(): # Check if defined
            print(f"‚ö†Ô∏è Progress possibly saved: {last_processed_index + 1}/{total_rows} rows processed.")
            print(f"‚ö†Ô∏è Run the script again to continue from row index {last_processed_index +1}.")
        else:
            print("‚ö†Ô∏è Could not determine last processed index due to early error.")


# Example of how to run the async function:
if __name__ == "__main__":
    # Ensure any global setup (like client init) is done before this
    input_csv = INPUT_CSV# Replace with your actual input file
    target_model = TEST_MODEL # Use the test model defined at the start
    
    # Check if input file exists before running
    if os.path.exists(input_csv):
        
        async def run_main_and_close_client():
            try:
                await only_q2_batch(input_csv, model=target_model)
            finally:
                await client.close() 

        asyncio.run(run_main_and_close_client())
    else:
        print(f"‚ùå Input file for processing not found: {input_csv}")

