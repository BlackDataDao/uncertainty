import pandas as pd
import csv
import time
import os
import certifi
# from openai import AuthenticationError, OpenAIError, OpenAI # OLD
from openai import AuthenticationError, OpenAIError, AsyncOpenAI # NEW
from datetime import datetime # Added import
import json # Added import for dumping messages
import re # Added import for regular expressions
import asyncio # NEW
from analyze_scores import analyze_role_scores
from filter_data import filter_random_data_by_product_type


# ─── Use certifi's CA bundle for all HTTPS requests ─────────────────────────
os.environ["REQUESTS_CA_BUNDLE"] = certifi.where()
os.environ["CURL_CA_BUNDLE"]     = certifi.where()

# ─── Load your OpenAI key from the environment ─────────────────────────────
TEST_MODEL = "deepseek-chat"  # Or "deepseek-chat", etc.

if TEST_MODEL == "gpt-4o-mini":
    hardcoded_api_key = "REMOVED"
    api_base_url = "https://api1.oaipro.com/v1"
elif TEST_MODEL == "deepseek-chat":
    hardcoded_api_key="REMOVED"
    api_base_url = "https://api.deepseek.com"
else:
    raise ValueError(f"Unsupported model: {TEST_MODEL}")


client = AsyncOpenAI( 
    api_key=hardcoded_api_key,
    base_url=api_base_url
)

# ─── Configuration ─────────────────────────────────────────────────────────

ROLES= [
    "stock broker",
    "stock broker whose role is to encourage clients to buy more crypto.",
    "stock broker whose role is to encourage clients to buy less crypto.",
    "risk manager",
    "risk manager whose role is to encourage clients to buy more crypto.",
    "risk manager whose role is to encourage clients to buy less crypto.",
]
ROLES= [
    "stock broker",
    "stock broker whose role is to encourage clients to buy more stocks.",
    "stock broker whose role is to encourage clients to buy less stocks.",
    "risk manager",
    "risk manager whose role is to encourage clients to buy more stocks.",
    "risk manager whose role is to encourage clients to buy less stocks.",
]
ROLES= [
    "stock broker",
    "risk manager",
]

SYSTEM_TEMPLATE = "You are a {role}."

Q1_TEMPLATE = (
    "Client profile:\n"
    "- Age: {age}\n"
    "- Net cash: ${net_cash:,.2f}\n\n"
    "He/she wants to invest {pct}% of net cash into {product} {product_type}.\n"
    "Please think step by step about whether he/she should do this. "
)

Q2_TEMPLATE = (
    "Based on the reasoning above, should he/she do the investment? "
    "Please provide a score from 1.000-10.000 (1.000 is lowest, 10.000 is highest recommendation) "
    "in the following format: INVESTMENT_SCORE: <score_value>\n"
    "For example: INVESTMENT_SCORE: 8.750"
    "finally , answer one word: Yes or No.(No means its risk level is high and not suitable for this client.)"
    "in the folowing format: INVESTMENT_RECOMMENDATION: <recommendation_value>\n"
    "For example: INVESTMENT_RECOMMENDATION: Yes"
)

INPUT_CSV_q1  = "groups\group1\dataset_test.csv" # q1 input file "groups\group1\dataset_test.csv"
INPUT_CSV_q2= "groups\group1\q1_gpt-4o-mini_dataset_test.csv" # q2 input file, should contain Q1 reasoning

def extract_score(text_content, strict=True):
    """
    Extracts a score (between 1.000 and 10.000) from a given text,
    prioritizing the format "INVESTMENT_SCORE: X.XXX".
    Handles bold markdown formatting (**).
    Returns the score as a float with 3 decimal places, or None if no score is found.
    """
    if not text_content:
        return None

    # Strict mode: only match explicit INVESTMENT_SCORE
    if strict:
        match = re.search(r"\*?INVESTMENT_SCORE\*?\s*:?\*?\s*(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)", text_content, re.IGNORECASE)
        if match:
            try:
                score = float(match.group(1))
                if 1.000 <= score <= 10.000:
                    return round(score, 3)
            except ValueError:
                pass
        return "N/A"

    # --- Fallback Patterns (less reliable) ---
    # Pattern 1: "score ... X.XXX" where X.XXX is 1.000-10.000
    match = re.search(r"\*?score\*?(?: is|:| of)?\s*(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)", 
                     text_content, 
                     re.IGNORECASE)
    if match:
        try:
            score = float(match.group(1))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    # Pattern 2: "X.XXX/10"
    match = re.search(r"(\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b)/10", text_content)
    if match:
        try:
            score = float(match.group(1))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    # Pattern 3: If the text contains just a decimal number between 1.000 and 10.000
    match = re.search(r"\b(?:10(?:\.000)?|[1-9](?:\.\d{1,3})?)\b", text_content)
    if match:
        try:
            score = float(match.group(0))
            if 1.000 <= score <= 10.000:
                return round(score, 3)
        except ValueError:
            pass

    return None


def extract_recommendation(text_content):
    """
    Extracts the Yes/No recommendation from text using the format "INVESTMENT_RECOMMENDATION: X".
    Handles bold markdown formatting (**) and various text formats.
    Returns the recommendation as a string ('Yes' or 'No'), or None if not found.
    """
    if not text_content:
        return None

    # Pattern 0: Prioritize "INVESTMENT_RECOMMENDATION: Yes/No" with optional bold markers
    # Matches: "INVESTMENT_RECOMMENDATION: Yes", "**INVESTMENT_RECOMMENDATION: Yes**", "**investment_recommendation:** Yes"
    priority_match = re.search(
        r"\*?INVESTMENT_RECOMMENDATION\*?\s*:?\*?\s*(Yes|No|YES|NO|yes|no)\*?",
        text_content,
        re.IGNORECASE
    )
    if priority_match:
        return priority_match.group(1).capitalize()

    # Pattern 1: Look for standalone "Yes" or "No" after "recommend" or "recommendation"
    recommend_match = re.search(
        r"recommend(?:ation)?(?:\s+is)?:\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if recommend_match:
        return recommend_match.group(1).capitalize()

    # Pattern 2: Look for "I/We recommend: Yes/No"
    direct_match = re.search(
        r"(?:I|we)\s+recommend:\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if direct_match:
        return direct_match.group(1).capitalize()

    # Pattern 3: Simple Yes/No at the end of the text
    final_match = re.search(
        r"(?:final\s+(?:recommendation|answer|decision)|conclusion):\s*(Yes|No|YES|NO|yes|no)\b",
        text_content,
        re.IGNORECASE
    )
    if final_match:
        return final_match.group(1).capitalize()

    return None

async def process_row_q2_concurrently(row_data, q2_model_name, api_client, semaphore, 
                                      q1_template_str, q2_template_str, system_template_str, roles_list_local):
    """
    Processes a single row to generate Q2 recommendations for all its roles concurrently.
    (Currently, ROLES has one item, so it's one API call per row, but structure supports more)
    """
    async with semaphore:
        processed_row_data = row_data.copy() # Work on a copy
        row_identifier = processed_row_data.get('id', f"index_{processed_row_data.get('original_index', 'N/A')}") # Get a unique ID or index for logging

        common_reasoning = processed_row_data.get("q1_reasoning", "")
        if not common_reasoning or "Error" in common_reasoning or "Authentication" in common_reasoning:
            print(f"⏭️ Skipping row (ID/Index: {row_identifier}) due to invalid or missing Q1 reasoning: '{str(common_reasoning)[:100]}...'") # ADDED PRINT
            # Mark error for all roles if Q1 reasoning is bad
            for role in roles_list_local:
                processed_row_data[f"{role}_recommend_new"] = "Skipped due to invalid Q1 reasoning"
                processed_row_data[f"{role}_score_new"] = "N/A"
                processed_row_data[f"{role}_recommendation_new"] = "N/A"
                processed_row_data[f"{role}_prompt_new"] = ""
                processed_row_data[f"{role}_temperature_new"] = 0.0
                processed_row_data[f"{role}_max_tokens_new"] = 0
                processed_row_data[f"{role}_top_p_new"] = 0.0
            return processed_row_data

        usr_msg_q1 = q1_template_str.format(
            age=processed_row_data.get("age", ""),
            gender=processed_row_data.get("gender", ""),
            net_cash=processed_row_data.get("net_cash", 0),
            pct=processed_row_data.get("percentage", ""),
            product=processed_row_data.get("product_name", ""),
            product_type=processed_row_data.get("product_type", ""),
        )

        for role in roles_list_local:
            sys_msg_q2 = system_template_str.format(role=role)
            
            messages_q2 = [
                {"role": "system", "content": sys_msg_q2},
                {"role": "user", "content": usr_msg_q1},
                {"role": "assistant", "content": common_reasoning},
                {"role": "user", "content": q2_template_str},
            ]
            temperature_q2 = 0.0
            max_tokens_q2 = 500
            top_p_q2 = 1.0

            try:
                # print(f"      Starting API call for role {role} in row (original data: {processed_row_data.get('id', 'N/A')})") # Debug
                resp2 = await api_client.chat.completions.create(
                    model=q2_model_name,
                    messages=messages_q2,
                    temperature=temperature_q2,
                    max_tokens=max_tokens_q2,
                    top_p=top_p_q2,
                )
                # print(f"      Finished API call for role {role} in row (original data: {processed_row_data.get('id', 'N/A')})") # Debug
                recommend_text = resp2.choices[0].message.content.strip()
                role_score = extract_score(recommend_text, strict=False)
                role_recommendation = extract_recommendation(recommend_text)

                processed_row_data[f"{role}_recommend_new"] = recommend_text.capitalize().replace("\n", "  ")
                processed_row_data[f"{role}_score_new"] = role_score
                processed_row_data[f"{role}_recommendation_new"] = role_recommendation
                processed_row_data[f"{role}_prompt_new"] = json.dumps(messages_q2)
                processed_row_data[f"{role}_temperature_new"] = temperature_q2
                processed_row_data[f"{role}_max_tokens_new"] = max_tokens_q2
                processed_row_data[f"{role}_top_p_new"] = top_p_q2

            except (AuthenticationError, OpenAIError, ConnectionError, TimeoutError, Exception) as e:
                error_msg = f"{type(e).__name__}: {e}"
                print(f"⚠️ API Error for role {role} in row (ID/Index: {row_identifier}): {error_msg}") # MODIFIED PRINT to use row_identifier
                processed_row_data[f"{role}_recommend_new"] = f"Error: {error_msg}"
                processed_row_data[f"{role}_score_new"] = "N/A"
                processed_row_data[f"{role}_recommendation_new"] = "N/A"
                processed_row_data[f"{role}_prompt_new"] = json.dumps(messages_q2)
                processed_row_data[f"{role}_temperature_new"] = temperature_q2
                processed_row_data[f"{role}_max_tokens_new"] = max_tokens_q2
                processed_row_data[f"{role}_top_p_new"] = top_p_q2
            
            # Small delay if making multiple calls for roles within the same row,
            # though semaphore manages overall concurrency.
            if len(roles_list_local) > 1:
                await asyncio.sleep(0.05) # Minimal delay
        
        return processed_row_data

async def process_row_q1_concurrently(row_data, q1_model_name, api_client, semaphore, 
                                      q1_template_str):
    """
    Processes a single row to generate Q1 reasoning.
    """
    async with semaphore:
        processed_row_data = row_data.copy()
        row_identifier = processed_row_data.get('id', f"index_{processed_row_data.get('original_index', 'N/A')}")

        usr_msg_q1 = q1_template_str.format(
            age=processed_row_data.get("age", ""),
            gender=processed_row_data.get("gender", ""),
            net_cash=processed_row_data.get("net_cash", 0),
            pct=processed_row_data.get("percentage", ""),
            product=processed_row_data.get("product_name", ""),
            product_type=processed_row_data.get("product_type", ""),
        )

        messages_q1 = [
            {"role": "user", "content": usr_msg_q1},
        ]
        temperature_q1 = 0.0
        max_tokens_q1 = 3000
        top_p_q1 = 1.0

        try:
            resp1 = await api_client.chat.completions.create(
                model=q1_model_name,
                messages=messages_q1,
                temperature=temperature_q1,
                max_tokens=max_tokens_q1,
                top_p=top_p_q1,
            )
            recommend_text = resp1.choices[0].message.content.strip()

            processed_row_data[f"q1_reasoning"] = recommend_text.capitalize().replace("\n", "  ")
            processed_row_data[f"q1_prompt"] = json.dumps(messages_q1)
            processed_row_data[f"q1_temperature"] = temperature_q1
            processed_row_data[f"q1_max_tokens"] = max_tokens_q1
            processed_row_data[f"q1_top_p"] = top_p_q1

        except (AuthenticationError, OpenAIError, ConnectionError, TimeoutError, Exception) as e:
            error_msg = f"{type(e).__name__}: {e}"
            print(f"⚠️ API Error in row (ID/Index: {row_identifier}): {error_msg}")
            processed_row_data[f"q1_reasoning"] = f"Error: {error_msg}"
            processed_row_data[f"q1_prompt"] = json.dumps(messages_q1)
            processed_row_data[f"q1_temperature"] = temperature_q1
            processed_row_data[f"q1_max_tokens"] = max_tokens_q1
            processed_row_data[f"q1_top_p"] = top_p_q1
        
        return processed_row_data

async def only_q2_batch(input_file,model,api_client): # Changed to async def
    """
    Uses existing Q1 reasoning from a previous output file to generate only Q2 recommendations
    concurrently.
    
    Args:
        input_file (str): Path to the input file containing previous Q1 results
        model (str): The model to use for Q2 recommendations
    """
    # Get the base name without extension
    input_base = os.path.splitext(os.path.basename(input_file))[0]
    # Extract the folder name from the input file path
    folder_name = os.path.dirname(input_file)

    output_csv_filename = os.path.join(folder_name, f"q2_{model}_{input_base}.csv") # Use os.path.join for cross-platform compatibility

    checkpoint_file = os.path.join(folder_name, f"checkpoint_{model}_q2.json") # Use os.path.join for cross-platform compatibility

    processed_rows_count_session = 0 # Renamed to avoid conflict
    total_rows = 0
    last_processed_index = -1
    
    CONCURRENCY_LIMIT = 10  # Number of concurrent API calls
    CHUNK_SIZE_FOR_PROCESSING = 50 # How many rows to gather tasks for before awaiting `gather`

    try:
        if not os.path.exists(input_file):
            print(f"❌ Error: File '{input_file}' not found")
            return

        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                last_processed_index = checkpoint.get('last_processed_index', -1)
                output_csv_filename = checkpoint.get('output_csv_filename', output_csv_filename)
                print(f"🔄 Resuming from row index {last_processed_index +1} (row number {last_processed_index + 2}) using file '{output_csv_filename}'")

        df = pd.read_csv(input_file, encoding="latin1") # Consider utf-8 if latin1 causes issues
        df['original_index'] = df.index # Add original index for better logging if 'id' is not present or unique
        all_rows_list_of_dicts = df.to_dict('records') # Keep as list of dicts
        total_rows = len(all_rows_list_of_dicts)
        print(f"🔍 Loaded {total_rows} rows from input file '{input_file}'.")

        q2_suffixes = [
            "_recommend", "_recommend_model", "_recommend_messages",
            "_recommend_temperature", "_recommend_max_tokens",
            "_score", "_recommendation"
        ]
        original_columns = [
            col for col in df.columns
            if not any(col.startswith(role) and any(col.endswith(suf) for suf in q2_suffixes) for role in ROLES)
        ]
        extra_fields = []
        for role in ROLES:
            extra_fields.extend([
                f"{role}_score_new",
                f"{role}_recommendation_new",
                f"{role}_recommend_new",
                f"{role}_prompt_new",
                f"{role}_temperature_new",
                f"{role}_max_tokens_new",
                f"{role}_top_p_new"
            ])
        output_fields = original_columns + extra_fields

        file_exists = os.path.isfile(output_csv_filename) and os.path.getsize(output_csv_filename) > 0
        
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        with open(output_csv_filename, "a" if file_exists else "w", newline="", encoding="utf-8") as fout:
            writer = csv.DictWriter(fout, fieldnames=output_fields, extrasaction='ignore') # Ignore extra keys not in output_fields
            
            if not file_exists:
                writer.writeheader()

            for i in range(last_processed_index + 1, total_rows, CHUNK_SIZE_FOR_PROCESSING):
                chunk_start_index = i
                chunk_end_index = min(i + CHUNK_SIZE_FOR_PROCESSING, total_rows)
                
                tasks = []
                # Prepare tasks for the current chunk
                for current_row_index in range(chunk_start_index, chunk_end_index):
                    row_to_process = all_rows_list_of_dicts[current_row_index]
                    tasks.append(
                        process_row_q2_concurrently(
                            row_to_process, model, api_client, semaphore,
                            Q1_TEMPLATE, Q2_TEMPLATE, SYSTEM_TEMPLATE, ROLES
                        )
                    )
                
                print(f"🚀 Processing chunk: rows from index {chunk_start_index} to {chunk_end_index -1} ({len(tasks)} tasks)...")
                # Execute tasks concurrently for the chunk
                # return_exceptions=True allows us to handle individual task failures
                processed_chunk_results = await asyncio.gather(*tasks, return_exceptions=True) 
                print(f"🏁 Chunk processed. Writing {len(processed_chunk_results)} results...")

                # Write results for the processed chunk
                for idx_in_chunk, result_or_exception in enumerate(processed_chunk_results):
                    actual_row_index = chunk_start_index + idx_in_chunk
                    
                    if isinstance(result_or_exception, Exception):
                        print(f"‼️ Critical error processing row index {actual_row_index} (task failed): {result_or_exception}")
                        # Write the original row with error markers if needed, or skip
                        # For now, we assume process_row_q2_concurrently handles internal errors and returns a dict
                        # This case is for unexpected failures in the task itself.
                        # Let's ensure the original row is written with some error indication if this happens.
                        error_marked_row = all_rows_list_of_dicts[actual_row_index].copy()
                        for role in ROLES: # Mark all roles as error
                             error_marked_row[f"{role}_recommend_new"] = f"TaskExecutionError: {result_or_exception}"
                        row_to_write = {k: v for k, v in error_marked_row.items() if k in output_fields}

                    else: # result_or_exception is the processed_row_data dict
                        # Update the main list (optional, if needed later, but good for consistency)
                        all_rows_list_of_dicts[actual_row_index].update(result_or_exception)
                        row_to_write = {k: v for k, v in all_rows_list_of_dicts[actual_row_index].items() if k in output_fields}

                    writer.writerow(row_to_write)
                    processed_rows_count_session += 1
                
                last_processed_index = chunk_end_index - 1 # Update to the last successfully initiated index in this chunk
                with open(checkpoint_file, 'w') as f_checkpoint:
                    json.dump({
                        'last_processed_index': last_processed_index,
                        'output_csv_filename': output_csv_filename,
                        'timestamp': str(datetime.now())
                    }, f_checkpoint)
                
                fout.flush()
                print(f"💾 Checkpoint saved. Last processed index: {last_processed_index}. Total rows processed this session: {processed_rows_count_session}")

        print(f"\n✅ Processed {processed_rows_count_session} rows this session.")
        print(f"✅ Progress: {last_processed_index + 1}/{total_rows} rows ({((last_processed_index + 1) / total_rows) * 100:.2f}%)")
        print(f"✅ Results saved to: {output_csv_filename}")

        if last_processed_index + 1 == total_rows:
            try:
                df_out = pd.read_csv(output_csv_filename, encoding="utf-8-sig") # Use utf-8-sig for potential BOM
                df_out.rename(
                    columns={col: col.replace('_new', '') for col in df_out.columns if col.endswith('_new')},
                    inplace=True
                )
                df_out.to_csv(output_csv_filename, index=False, encoding="utf-8") # Save as utf-8
                print(f"🎉 Column names '_new' suffix removed from '{output_csv_filename}'.")
                os.remove(checkpoint_file)
                print(f"🎉 All rows processed successfully! Checkpoint file '{checkpoint_file}' removed.")
                # Analyze scores after final processing
                print("🔍 Analyzing scores...")

            except FileNotFoundError:
                 print(f"⚠️ Checkpoint file '{checkpoint_file}' not found for removal (might be first run or already removed).")
            except Exception as e:
                print(f"⚠️ Error during final cleanup (removing '_new' or checkpoint): {e}")


    except KeyboardInterrupt:
        print("\n⚠️ Process interrupted by user.")
        print(f"⚠️ Progress saved: {last_processed_index + 1}/{total_rows} rows processed.")
        print(f"⚠️ Run the script again to continue from row index {last_processed_index +1}.")
    except Exception as e:
        print(f"\n❌ Critical error in main_only_recommend_batch: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        if 'last_processed_index' in locals(): # Check if defined
            print(f"⚠️ Progress possibly saved: {last_processed_index + 1}/{total_rows} rows processed.")
            print(f"⚠️ Run the script again to continue from row index {last_processed_index +1}.")
        else:
            print("⚠️ Could not determine last processed index due to early error.")
    return output_csv_filename

async def only_q1_batch(input_file, model,api_client):
    """
    Generates Q1 reasoning for data from a CSV file concurrently.
    
    Args:
        input_file (str): Path to the input CSV file.
        model (str): The model to use for Q1 reasoning.
    """
    # Extract the folder name from the input file path
    folder_name = os.path.dirname(input_file)
    if folder_name:
        input_base = os.path.splitext(os.path.basename(input_file))[0]
    else:
        input_base = os.path.splitext(input_file)[0]

    output_csv_filename= os.path.join(folder_name, f"q1_{model}_{input_base}.csv") # Use os.path.join for cross-platform compatibility
    # output_csv_filename = f"q1_{model}_{input_base}.csv"
    checkpoint_file = os.path.join(folder_name, f"checkpoint_{model}_q1.json")

    processed_rows_count_session = 0
    total_rows = 0
    last_processed_index = -1
    
    CONCURRENCY_LIMIT = 10  # Number of concurrent API calls
    CHUNK_SIZE_FOR_PROCESSING = 50 # How many rows to gather tasks for before awaiting `gather`

    try:
        if not os.path.exists(input_file):
            print(f"❌ Error: File '{input_file}' not found")
            return

        if os.path.exists(checkpoint_file):
            with open(checkpoint_file, 'r') as f:
                checkpoint = json.load(f)
                last_processed_index = checkpoint.get('last_processed_index', -1)
                output_csv_filename = checkpoint.get('output_csv_filename', output_csv_filename)
                print(f"🔄 Resuming from row index {last_processed_index +1} (row number {last_processed_index + 2}) using file '{output_csv_filename}'")

        df = pd.read_csv(input_file, encoding="latin1") # Consider utf-8 if latin1 causes issues
        df['original_index'] = df.index # Add original index for better logging if 'id' is not present or unique
        all_rows_list_of_dicts = df.to_dict('records') # Keep as list of dicts
        total_rows = len(all_rows_list_of_dicts)
        print(f"🔍 Loaded {total_rows} rows from input file '{input_file}'.")

        original_columns = df.columns.tolist()
        extra_fields = [
                f"q1_reasoning",
                f"q1_prompt",
                f"q1_temperature",
                f"q1_max_tokens",
                f"q1_top_p"
            ]
        output_fields = original_columns + extra_fields

        file_exists = os.path.isfile(output_csv_filename) and os.path.getsize(output_csv_filename) > 0
        
        semaphore = asyncio.Semaphore(CONCURRENCY_LIMIT)

        with open(output_csv_filename, "a" if file_exists else "w", newline="", encoding="utf-8") as fout:
            writer = csv.DictWriter(fout, fieldnames=output_fields, extrasaction='ignore') # Ignore extra keys not in output_fields
            
            if not file_exists:
                writer.writeheader()

            for i in range(last_processed_index + 1, total_rows, CHUNK_SIZE_FOR_PROCESSING):
                chunk_start_index = i
                chunk_end_index = min(i + CHUNK_SIZE_FOR_PROCESSING, total_rows)
                
                tasks = []
                # Prepare tasks for the current chunk
                for current_row_index in range(chunk_start_index, chunk_end_index):
                    row_to_process = all_rows_list_of_dicts[current_row_index]
                    tasks.append(
                        process_row_q1_concurrently(
                            row_to_process, model, api_client, semaphore,
                            Q1_TEMPLATE
                        )
                    )
                
                print(f"🚀 Processing chunk: rows from index {chunk_start_index} to {chunk_end_index -1} ({len(tasks)} tasks)...")
                # Execute tasks concurrently for the chunk
                # return_exceptions=True allows us to handle individual task failures
                processed_chunk_results = await asyncio.gather(*tasks, return_exceptions=True) 
                print(f"🏁 Chunk processed. Writing {len(processed_chunk_results)} results...")

                # Write results for the processed chunk
                for idx_in_chunk, result_or_exception in enumerate(processed_chunk_results):
                    actual_row_index = chunk_start_index + idx_in_chunk
                    
                    if isinstance(result_or_exception, Exception):
                        print(f"‼️ Critical error processing row index {actual_row_index} (task failed): {result_or_exception}")
                        error_marked_row = all_rows_list_of_dicts[actual_row_index].copy()
                        error_marked_row[f"q1_reasoning"] = f"TaskExecutionError: {result_or_exception}"
                        row_to_write = {k: v for k, v in error_marked_row.items() if k in output_fields}

                    else: # result_or_exception is the processed_row_data dict
                        # Update the main list (optional, if needed later, but good for consistency)
                        all_rows_list_of_dicts[actual_row_index].update(result_or_exception)
                        row_to_write = {k: v for k, v in all_rows_list_of_dicts[actual_row_index].items() if k in output_fields}

                    writer.writerow(row_to_write)
                    processed_rows_count_session += 1
                
                last_processed_index = chunk_end_index - 1 # Update to the last successfully initiated index in this chunk
                with open(checkpoint_file, 'w') as f_checkpoint:
                    json.dump({
                        'last_processed_index': last_processed_index,
                        'output_csv_filename': output_csv_filename,
                        'timestamp': str(datetime.now())
                    }, f_checkpoint)
                
                fout.flush()
                print(f"💾 Checkpoint saved. Last processed index: {last_processed_index}. Total rows processed this session: {processed_rows_count_session}")

        print(f"\n✅ Processed {processed_rows_count_session} rows this session.")
        print(f"✅ Progress: {last_processed_index + 1}/{total_rows} rows ({((last_processed_index + 1) / total_rows) * 100:.2f}%)")
        print(f"✅ Results saved to: {output_csv_filename}")
        
        if last_processed_index + 1 == total_rows:
            try:
                print(f"🎉 All rows processed successfully! Checkpoint file '{checkpoint_file}' removed.")
                os.remove(checkpoint_file)
            except FileNotFoundError:
                 print(f"⚠️ Checkpoint file '{checkpoint_file}' not found for removal (might be first run or already removed).")
            except Exception as e:
                print(f"⚠️ Error during final cleanup (removing checkpoint): {e}")


    except KeyboardInterrupt:
        print("\n⚠️ Process interrupted by user.")
        print(f"⚠️ Progress saved: {last_processed_index + 1}/{total_rows} rows processed.")
        print(f"⚠️ Run the script again to continue from row index {last_processed_index +1}.")
    except Exception as e:
        print(f"\n❌ Critical error in only_q1_batch: {type(e).__name__}: {e}")
        import traceback
        traceback.print_exc()
        if 'last_processed_index' in locals(): # Check if defined
            print(f"⚠️ Progress possibly saved: {last_processed_index + 1}/{total_rows} rows processed.")
            print(f"⚠️ Run the script again to continue from row index {last_processed_index +1}.")
        else:
            print("⚠️ Could not determine last processed index due to early error.")

    return output_csv_filename 

async def async_whole_process(folder_name, model="gpt-4o-mini",dataset_num=1,asset_types=["crypto", "stock"]):
    """
    Asynchronous main process to run Q1, Q2, and analysis
    using a single, safely managed client instance.
    """
    # Use 'async with' to manage the client's lifecycle automatically.
    # The client is created here and automatically closed at the end of the block.
    async with AsyncOpenAI(api_key=hardcoded_api_key, base_url=api_base_url) as client:
        
        # --- Q1 Processing ---
        Q1_INPUT_CSV = filter_random_data_by_product_type(dataset_num, asset_types, folder_name)
        
        q2_input_file = None
        if os.path.exists(Q1_INPUT_CSV):
            print(f"--- Running Q1 process for: {Q1_INPUT_CSV} ---")
            # Pass the already created client to the batch function
            q1_output_files = await only_q1_batch(Q1_INPUT_CSV, model=model, api_client=client)
            if q1_output_files:
                q2_input_file = q1_output_files # Get the first (and only) output file
        else:
            print(f"❌ Input file for Q1 not found: {Q1_INPUT_CSV}")

        # --- Q2 Processing ---
        # Use the output from Q1 as the input for Q2
        if q2_input_file and os.path.exists(q2_input_file):
            print(f"--- Running Q2 process for: {q2_input_file} ---")
            # Pass the same client to the Q2 batch function
            q2_output_file = await only_q2_batch(q2_input_file, model=model, api_client=client)
        else:
            print(f"❌ Input file for Q2 not generated or found. Skipping Q2 and analysis.")
            q2_output_file = None

        # --- Analysis ---
        if q2_output_file and os.path.exists(q2_output_file):
            print(f"--- Analyzing results from: {q2_output_file} ---")
            analyze_role_scores(q2_output_file, roles=ROLES,
                                output_folder_path=folder_name
            )
    # The client is now automatically and safely closed.
    print(f"\n✅ Complete. Results saved in folder: {folder_name}")


def whole_process(folder_name, model, dataset_num=1, asset_types=["crypto", "stock"]):
    # This is the only place you need asyncio.run()
    asyncio.run(async_whole_process(folder_name, model=model, dataset_num=dataset_num, asset_types=asset_types))


# --- Modify your batch functions to accept a client ---
# You need to make a small change to only_q1_batch and only_q2_batch
# so they don't use the global client anymore.


# --- Final call at the end of your script ---
if __name__ == "__main__":
    whole_process("groups_deepseek/group1/", TEST_MODEL, dataset_num=200, asset_types=["crypto", "stock"])


